{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 极客讲坛-传统机器学习\n",
    "\n",
    "和树繁，19人工智能专业。\n",
    "\n",
    "- QQ：953765148\n",
    "- Phone: 15634329859"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "### 什么是机器学习\n",
    "\n",
    "学习：广义的学习是指人与动物在生活过程中凭借经验产生的行为或行为潜能的相对持久的变化。——摘自百度百科\n",
    "\n",
    "那么机器学习可以理解为：机器在计算过程中提高性能的自我改进的过程。\n",
    "\n",
    "- 关于“机器学习”这个词也有其他解释。\n",
    "\n",
    "从功能方面来看，机器学习是指计算机自动从数据中发现“规律”（或者说是“知识”），并且将学习到的“规律”应用到新的问题的过程。\n",
    "\n",
    "举例：垃圾邮件识别。给定之前收到过的一些邮件，第$i$个邮件记作$M_i$；由人工手动标记每个邮件是否为垃圾邮件，记第$i$个邮件的标记为$y_i$，垃圾邮件标记$y_i=1$，非垃圾邮件标记$y_i=0$。机器学习算法通过已知的$(M_i, y_i)$数据对，学习一个函数$F(M)->y$，作用为输入一个邮件$M$输出其标签$y$。然后将学习到的这个函数$F$应用到**新的邮件中**。\n",
    "\n",
    "在上述例子中，数据即为已知的$(M_i, y_i)$数据对，“规律”为函数$F$，新的问题为“预测新的邮件是否为垃圾邮件”。\n",
    "\n",
    "### 机器学习的发展阶段\n",
    "\n",
    "1. 第一阶段在20世纪50年代中期到60年代中期，属于热烈时期。其主要研究方法为不断改进系统的控制参数，提升系统的执行能力。这里面不涉及到与具体任务有关的知识。\n",
    "2. 第二阶段在20世纪60年代中期到70年代中期，被称为冷静时期。主要研究方法为模拟人类的概念学习阶段，采用逻辑结构或图结构作为机器内部描述。\n",
    "3. 第三阶段在20世纪70年代中期到80年代中期，被称为复兴时期。主要研究方法从学习单个概念扩展到学习多个概念，探索不同学习策略和方法。\n",
    "4. 第四阶段始于1986年。这一阶段机器学习称为新的学课，并且逐渐开始兴起各种学习方法（如后面的深度学习）。\n",
    "\n",
    "我们把基于深度神经网络模型的方法叫做深度学习，其他学习方法称为传统机器学习。来区分本次的内容。\n",
    "\n",
    "### 解决问题的框架\n",
    "\n",
    "1. 基于规则的框架。人从问题中发现规律，然后写出可以形式化的规则，最后通过该规则处理每个数据。\n",
    "2. 基于学习的框架。设计一个**学习算法**，通过训练数据运行学习算法得到一个模型。通过这个模型处理每个新的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传统机器学习\n",
    "\n",
    "### 机器学习的三要素\n",
    "\n",
    "1. 模型。用来描述问题的规律。\n",
    "2. 策略。如何评价模型的好坏。\n",
    "3. 算法。如何调整模型的参数。\n",
    "\n",
    "### 任务分类\n",
    "\n",
    "按照其训练数据的不同类型可以分为：\n",
    "\n",
    "1. 有监督学习。指每个数据有输入数据和标准输出结果。\n",
    "2. 无监督学习。指每个数据只有输入数据，没有标准输出结果。\n",
    "3. 其他。在深度学习快速发展的现在，有半监督学习、弱监督学习等等。\n",
    "\n",
    "按照所处理的任务不同可以分为：\n",
    "\n",
    "1. 分类模型。模型需要给出每个输入数据所属的类别。例如垃圾邮件分类，猫狗分类。\n",
    "2. 聚类模型。模型需要给出每个输入所处的类，使得类内相似度高、类间相似度低。\n",
    "3. 回归模型。模型需要给出每个输入对应的输出数字。例如股票价格预测（今年美赛）。\n",
    "4. 其他模型。例如策略模型（对应强化学习），不在本次讲座范围内。\n",
    "\n",
    "### 讲座内容\n",
    "\n",
    "我的部分主要讲解几个传统的而且常用的算法。规则学习部分、深度学习涉及较少。讲解内容及其对应的功能分类如下：\n",
    "\n",
    "1. **K近邻算法。分类模型，有监督学习。**\n",
    "2. **线性回归。回归模型，有监督学习。**\n",
    "3. **K-means算法。聚类模型，无监督学习。**\n",
    "4. **决策树模型。分类模型，有监督学习。**\n",
    "5. **贝叶斯算法。分类模型，有监督学习。**\n",
    "\n",
    "上述算法可以是传统机器学习的入门级算法，**可以课上大家一起做一做**。\n",
    "\n",
    "但机器学习远不止这些算法。大家如果感兴趣的话，可以去了解一下SVM、DBSCAN、模拟退火、遗传算法、PCA和LDA、AQ15等算法。由于时间原因此处就不讲了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K近邻算法\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "该模型假设：同一个类别的不同样本在样本空间的分布临近。\n",
    "\n",
    "- 样本空间：随机试验E的所有基本结果组成的集合为E的样本空间。样本空间的元素称为样本点或基本事件。此处的随机试验也就是不同采样。\n",
    "\n",
    "由上述假设可以引申出：对于一个新的待分类样本，搜索其最近的K个已知样本点，按照样本点所属类别来确定该待分类样本所属的类别。\n",
    "\n",
    "### 核心代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "class KNN:\n",
    "    \"\"\"\n",
    "    KNN算法的核心代码。\n",
    "    此处所有数据不采用numpy形式，采用数组的形式存储。\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data, train_labels, K=5):\n",
    "        \"\"\"\n",
    "        train_data: shape [N, D]. type float.\n",
    "        train_labels: shape [N]. type any.\n",
    "        K: int.\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.K = K\n",
    "        self.N = len(train_data)\n",
    "    \n",
    "    def calc_distance(self, x, y):\n",
    "        \"\"\"\n",
    "        这里采用了平方差距离作为距离度量函数，与欧氏距离相比不开根号了，计算更快。\n",
    "        distance of x (shape [D]) and y (shape [D]).\n",
    "        return: float.\n",
    "        \"\"\"\n",
    "        distance = sum((x[i] - y[i])**2 for i in range(len(x)))\n",
    "        return distance\n",
    "    \n",
    "    def test(self, x):\n",
    "        \"\"\"\n",
    "        x: shape [D]. type float.\n",
    "        return: int.\n",
    "        \"\"\"\n",
    "        distance_label_list = []\n",
    "        # 计算不同样本点的距离\n",
    "        for i in range(self.N):\n",
    "            distance = self.calc_distance(self.train_data[i], x)\n",
    "            distance_label_list.append((distance, self.train_labels[i]))\n",
    "        # 排序求距离小的前K个\n",
    "        distance_label_list.sort(key=lambda x: x[0])\n",
    "        distance_label_list = distance_label_list[: self.K]\n",
    "        # 统计不同类别出现的次数\n",
    "        label_count_map = {}\n",
    "        for x, y in distance_label_list:\n",
    "            label_count_map[y] = label_count_map.get(y, 0) + 1\n",
    "        # 输出类别\n",
    "        label_count_list = list(label_count_map.items())\n",
    "        label_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        return label_count_list[0][0]\n",
    "\n",
    "\n",
    "train_data = [\n",
    "    [0, 0], [0, 1], [1, 0],\n",
    "    [0, 2], [0, 3], [1, 3],\n",
    "    [2, 0], [3, 0], [3, 1],\n",
    "    [3, 2], [3, 3], [2, 3],\n",
    "]\n",
    "train_labels = [\n",
    "    0, 0, 0,\n",
    "    1, 1, 1,\n",
    "    2, 2, 2,\n",
    "    3, 3, 3,\n",
    "]\n",
    "model = KNN(train_data, train_labels, 3)\n",
    "test_data = [1, 1]\n",
    "print(model.test(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点与改进\n",
    "\n",
    "**优点**：写起来非常简单，而且不需要进行额外的参数估计（只需要存储训练数据）\n",
    "\n",
    "**缺点**：计算复杂度方面，训练数据多的时候，预测新类别计算量大；准确度层面，训练数据不均衡的时候，预测结果可能会有偏差；存储方面，需要存储全量的训练数据。\n",
    "\n",
    "- 训练数据不均衡：这个词的意思时不同类别的训练样本数量差异大。\n",
    "\n",
    "**一些改进方法**\n",
    "\n",
    "1. 针对计算复杂度方面，可以将原始样本按照近邻关系分组，分成不同的区域。每次搜索的时候在对应的区域内进行近邻搜索。\n",
    "2. 针对存储问题，尝试压缩训练数据集。具体的方法可以去查。\n",
    "3. 其他优化，如建立K-D tree等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "顾名思义，一个线性的回归方程。\n",
    "\n",
    "$$\n",
    "\\hat y = \\vec a * \\vec x + b = \\sum_{i=0}^{d-1}( a_i * x_i ) + b = a_0 * x_0 + a_1 * x_1 + ... + a_{d-1} * x_{d-1} +b\n",
    "$$\n",
    "\n",
    "### 适用条件\n",
    "\n",
    "在$x$与$y$是具有相关关系的两个变量，且$n$个观测值的$n$个点大致分布在一条直线附近的条件下，我们可以使用线性回归求在整体上与这n个点最接近的一条直线。\n",
    "\n",
    "**相关关系：**\n",
    "\n",
    "可以采用相关系数衡量。其计算公式为：\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar x)^2 \\sum_{i=1}^n(y_i - \\bar y)^2}}\n",
    "$$\n",
    "\n",
    "且$|r|$越接近1，相关程度越大；$|r|$越接近0，相关程度越小。\n",
    "\n",
    "### 评价策略\n",
    "\n",
    "如何衡量整体上最接近？常见的度量方法是平方误差。\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - \\hat y_i)^2\n",
    "$$\n",
    "\n",
    "当然，除此之外你也可以选择其他的度量函数，如$log$距离函数，绝对值度量函数等。\n",
    "\n",
    "### 训练算法\n",
    "\n",
    "以平方误差度量为例，求解最优条件下的参数$a_i, b$。其优化过程可以记作：\n",
    "\n",
    "$$\n",
    "\\vec a, b = \\textbf{argmin}_{\\vec a, b} \\sum_{i=1}^n(y_i - ( \\vec a * \\vec x_i + b))^2\n",
    "$$\n",
    "\n",
    "求解上述方程的最小值时参数的值，就得到了最优参数。\n",
    "\n",
    "在$x$只有**一维的条件下**，采用最小二乘法可以求解出最优解如下：\n",
    "\n",
    "$$\n",
    "b = \\frac{\\sum_{i=1}^n (x_i-\\bar x) (y_i - \\bar y)}{\\sum_{i=1}^n (x_i-\\bar x)^2} \\\\\n",
    "a = \\bar y - b \\bar x\n",
    "$$\n",
    "\n",
    "高维条件下，也可以进行估计。但其计算过程较为复杂。可以参考：[多元线性回归参数估计](https://zhuanlan.zhihu.com/p/91095053)\n",
    "\n",
    "**有没有其他的更适用一些的训练方法？有！**\n",
    "\n",
    "### 梯度下降方法训练线性回归\n",
    "\n",
    "梯度下降是什么？ 它是以评价函数为基础，向评价函数值减小最快的方向调整参数。\n",
    "\n",
    "**值减小最快的方向：梯度的反方向。**\n",
    "\n",
    "以多元回归为例，优化下述的评价函数。\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n(y_i - ( \\vec a * \\vec x_i + b))^2\n",
    "$$\n",
    "\n",
    "对$\\vec a, b$求偏导：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial a_k} = \\sum_{i=1}^n 2 * (y_i - (\\vec a * \\vec x_i + b)) * (- x_{ik}) \\\\\n",
    "\\frac{\\partial z}{\\partial b} = \\sum_{i=1}^n 2 * (y_i - (\\vec a * \\vec x_i + b)) * (-1)\n",
    "$$\n",
    "\n",
    "可以等价写成：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial a_k} = \\sum_{i=1}^n 2 * x_{ik} * (\\hat y_i - y_i) \\\\\n",
    "\\frac{\\partial z}{\\partial b} = \\sum_{i=1}^n 2 * (\\hat y_i - y_i)\n",
    "$$\n",
    "\n",
    "每次调整参数为$u*\\partial()$，其中$u$是一个系数。\n",
    "\n",
    "\n",
    "### 基于梯度下降的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0, loss 142.10421225427925\n",
      "round 10, loss 0.17275615081780765\n",
      "round 20, loss 0.05970301794423334\n",
      "round 30, loss 0.020632841926471763\n",
      "round 40, loss 0.00713053009079708\n",
      "round 50, loss 0.0024642489656518966\n",
      "round 60, loss 0.0008516229350960389\n",
      "round 70, loss 0.0002943134535880668\n",
      "round 80, loss 0.00010171216085574863\n",
      "round 90, loss 3.515083506994019e-05\n",
      "round 100, loss 1.2147821811273495e-05\n",
      "[2.000212855167453] 0.9979852691016\n"
     ]
    }
   ],
   "source": [
    "class MLR:\n",
    "    \"\"\"\n",
    "    Multiple Linear Regression，基于梯度下降和平方误差的实现。\n",
    "    使用list作为计算基本单位。\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim):\n",
    "        \"\"\"\n",
    "        x_dim是输入维度\n",
    "        \"\"\"\n",
    "        self.x_dim = x_dim\n",
    "        self.a = [0] * x_dim\n",
    "        self.b = 0\n",
    "        \n",
    "    def test(self, x):\n",
    "        hat_y = self.b\n",
    "        for i in range(self.x_dim):\n",
    "            hat_y += self.a[i] * x[i]\n",
    "        return hat_y\n",
    "    \n",
    "    def update(self, x, y, u=0.01):\n",
    "        hat_y = self.test(x)\n",
    "        self.b -= u * 2 * (hat_y - y)\n",
    "        for k in range(self.x_dim):\n",
    "            self.a[k] -= u * 2 * x[k] * (hat_y - y)\n",
    "        return (hat_y - y) ** 2\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(10):\n",
    "    train_x.append((i, ))\n",
    "    train_y.append(2 * i + 1)\n",
    "\n",
    "model = MLR(1)\n",
    "for i in range(0, 101):\n",
    "    sum_loss = 0\n",
    "    for j in range(10):\n",
    "        sum_loss += model.update(train_x[j], train_y[j])\n",
    "    if i % 10 == 0:\n",
    "        print('round {}, loss {}'.format(i, sum_loss))\n",
    "\n",
    "print(model.a, model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展与改进\n",
    "\n",
    "统计软件中有非常丰富的多元回归工具包，如SPSS等软件。大家可以直接将数据输入然后点击运行就可以得到对应的参数内容。\n",
    "\n",
    "除经典的评价函数外，你也可以使用$sigmoid$函数的差作为评价函数，然后只需要改$update$即可。当然，你也可以转化评价函数为常规的平方和评价函数，然后继续套用上面的最小二乘估计。\n",
    "\n",
    "除一次多元回归之外，针对$y=a*x^2$的情况，可以引入一个新的辅助变量叫做$x^2$，继续进行线性回归。其他情况也可以通过引入辅助自变量的形式来满足线性回归的条件。**不过多元线性回归中不能出现多个变量线性相关的情况。**\n",
    "\n",
    "线性回归使用前需要先进行**相关性检验**，对于不满足线性相关的数据，你用了线性回归也没有意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means聚类算法\n",
    "\n",
    "### 什么是聚类任务\n",
    "\n",
    "聚类就是按照某个特定标准(如距离准则)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同数据尽量分离。\n",
    "\n",
    "简单地说，**聚类就是把相似的东西分到一组**。\n",
    "\n",
    "### K-means的优化目标\n",
    "\n",
    "假设我们提取到原始数据的集合为$D(x_1, x_2, …, x_n)$，并且每个$x_i$为$d$维的向量。\n",
    "\n",
    "K-means聚类的目的就是，在给定分类组数$k(k \\leq n)$值的条件下，将原始数据分成$k$类，$S={S_1, S_2, …, S_k}$。\n",
    "\n",
    "在数值模型上，即对以下表达式求最小值：\n",
    "\n",
    "$$\n",
    "\\textbf{argmin}_{S}\\sum_{i=1}^k \\sum_{x_j\\in S_i} ||x_j - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "其中$\\mu_i$为分组$S_i$的平均值。\n",
    "\n",
    "### K-means的优化过程\n",
    "\n",
    "1. 从D中随机取k个元素，作为k个簇的各自的中心。\n",
    "2. 分别计算剩下的元素到k个簇中心的相异度，将这些元素分别划归到相异度最低的簇。\n",
    "3. 根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均数。\n",
    "4. 将D中全部元素按照新的中心重新聚类。\n",
    "5. 重复第3-4步，直到聚类结果不再变化。\n",
    "6. 将结果输出。\n",
    "\n",
    "注：K-means聚类算法是EM算法的一个特例，有兴趣的同学可以去了解一下适用性很强的EM算法。\n",
    "\n",
    "### K-means核心代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data [0, 1], label 1\n",
      "Data [0, -1], label 1\n",
      "Data [1, 0], label 1\n",
      "Data [-1, 0], label 1\n",
      "Data [5, 6], label 0\n",
      "Data [5, 4], label 0\n",
      "Data [6, 5], label 0\n",
      "Data [4, 5], label 0\n"
     ]
    }
   ],
   "source": [
    "class Kmeans:\n",
    "    \"\"\"\n",
    "    Kmeans基础代码，所有计算基于list\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, K):\n",
    "        \"\"\"\n",
    "        初始化聚类中心的存储。\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.x_dim = x_dim\n",
    "        self.center = [[0 for i in range(x_dim)] for j in range(K)]\n",
    "    \n",
    "    def calc_distance(self, x, y):\n",
    "        distance = sum((x[i]-y[i])**2 for i in range(len(x)))\n",
    "        return distance\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        训练聚类中心\n",
    "        \"\"\"\n",
    "        N = len(train_data)\n",
    "        assert N >= self.K\n",
    "        label = [-1] * N\n",
    "        \n",
    "        # 随机寻找K个中心点\n",
    "        for i in range(self.K):\n",
    "            self.center[i] = train_data[i]\n",
    "        have_change = True\n",
    "        \n",
    "        # 重复Kmeans过程直到收敛\n",
    "        while have_change:\n",
    "            have_change = False\n",
    "            \n",
    "            # 计算所属分类\n",
    "            for i in range(N):\n",
    "                min_distance = self.calc_distance(train_data[i], self.center[0])\n",
    "                min_label = 0\n",
    "                for j in range(1, self.K):\n",
    "                    distance = self.calc_distance(train_data[i], self.center[j])\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        min_label = j\n",
    "                if min_label != label[i]:\n",
    "                    label[i] = min_label\n",
    "                    have_change = True\n",
    "            \n",
    "            # 计算类别中心\n",
    "            label_count = [0] * self.K\n",
    "            self.center = [[0 for i in range(self.x_dim)] for j in range(self.K)]\n",
    "            for i in range(N):\n",
    "                label_count[label[i]] += 1\n",
    "                self.center[label[i]] = list(self.center[label[i]][j] + train_data[i][j] for j in range(self.x_dim))\n",
    "            for i in range(self.K):\n",
    "                self.center[i] = list(self.center[i][j] / label_count[i] for j in range(self.x_dim))\n",
    "        \n",
    "    def test(self, x):\n",
    "        \"\"\"\n",
    "        测试聚类结果\n",
    "        \"\"\"\n",
    "        min_distance = self.calc_distance(x, self.center[0])\n",
    "        min_label = 0\n",
    "        for j in range(1, self.K):\n",
    "            distance = self.calc_distance(x, self.center[j])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                min_label = j\n",
    "        return min_label\n",
    "\n",
    "train_data = [\n",
    "    [0, 1], [0, -1], [1, 0], [-1, 0],\n",
    "    [5, 6], [5, 4], [6, 5], [4, 5],\n",
    "]\n",
    "model = Kmeans(2, 2)\n",
    "model.train(train_data)\n",
    "for x in train_data:\n",
    "    print('Data {}, label {}'.format(x, model.test(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展与改进\n",
    "\n",
    "Kmeans算法计算简单，逻辑清晰。但聚类的个数$K$需要提前人工给定。因此$K$的选择是一个比较大的问题。\n",
    "\n",
    "不过，Kmeans算法是聚类算法的一种。除此之外还有层次聚类、密度聚类的算法可以选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树算法\n",
    "\n",
    "### 从一个例子看起\n",
    "\n",
    "以下图所示的数据为例：\n",
    "\n",
    "![决策树数据示例](https://pic3.zhimg.com/v2-ed38beb4538a90f2b961233b18acc1ca_r.jpg)\n",
    "\n",
    "如果我们学习如何要区分“好学生”和“非好学生”，那么我们可以通过建立这样的一棵树形结构来判定：\n",
    "\n",
    "![树形结构1](https://pic1.zhimg.com/80/v2-ff4fe0d16ec17c5520837b3aad52ed54_720w.jpg)\n",
    "\n",
    "当然你也可以构造这样一个树：\n",
    "\n",
    "![树形结构2](https://pic3.zhimg.com/80/v2-8f6407e5ab5a58b2913aef6a332090f6_720w.jpg)\n",
    "\n",
    "**之后每来一个新的学生样本，你都可以在树上进行分类，最终判断该样本是否属于好学生样本。**\n",
    "\n",
    "随之而来的就有这么几个问题：\n",
    "\n",
    "1. 如何选择每个节点所使用的分类属性？\n",
    "2. 如何让构造的树规模尽量小？（泛化性更强）\n",
    "\n",
    "### ID3决策树\n",
    "\n",
    "ID3决策树学习算法采用**信息熵**来选择分类的属性。\n",
    "\n",
    "1. 啥是信息熵？\n",
    "\n",
    "信息熵是信息论中的概念。其计算公式为：\n",
    "\n",
    "$$\n",
    "- \\sum_{i} p_i * \\log(p_i)\n",
    "$$\n",
    "\n",
    "其中$i$表示数据中的不同类别。这里需要知道的是，熵越小说明分类结果越好。如果数据中仅含有一种类别，则其信息熵计算如下：\n",
    "\n",
    "$$\n",
    "- (1.0 * \\log(1.0)) = 0\n",
    "$$\n",
    "\n",
    "如果含有两类，且分别占50%，总体信息熵计算如下：\n",
    "\n",
    "$$\n",
    "- (0.5 * \\log(0.5) + 0.5 * \\log(0.5)) = - \\log(0.5) > 0\n",
    "$$\n",
    "\n",
    "2. 如何利用信息熵来选择属性？\n",
    "\n",
    "思想：尝试不同的分类属性，计算选择该属性后带来的信息增益，选择信息增益最大的属性作为分类属性。\n",
    "\n",
    "假设当前节点所含数据为$S_0$，选择属性$F$后分成了不同的子数据集合$S_1, S_2, ..., S_k$，其中每个集合的信息熵计算为：\n",
    "\n",
    "$$\n",
    "I(S) = - \\sum_{i\\in 所有类别}(p_i * \\log(p_i))\n",
    "$$\n",
    "\n",
    "则分类后的信息增益计算公式为：\n",
    "\n",
    "$$\n",
    "gain(F) = (\\sum_{i=1}^{k} I(S_i) )- I(S_0)\n",
    "$$\n",
    "\n",
    "最终选择$gain(F)$最大的$F$作为当前的分类属性。\n",
    "\n",
    "### ID3决策树代码\n",
    "\n",
    "[代码有点长，这是我自己的博客上的内容](https://sofanhe.blog.csdn.net/article/details/117480978)\n",
    "\n",
    "### 扩展\n",
    "\n",
    "ID3决策树对噪声数据的容忍度很低，归纳能力不够强，速度也相对较慢。\n",
    "\n",
    "后续出现了很多算法，例如**C5.0决策树算法**，引入了一些剪枝、合并操作，提高了归纳能力、加快了推理速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯算法\n",
    "\n",
    "贝叶斯算法是基于条件概率的分类方法，其包含朴素贝叶斯、贝叶斯信念网络等方法。这里讲的是朴素贝叶斯方法。\n",
    "\n",
    "### 朴素贝叶斯的思想\n",
    "\n",
    "对于数据$X$和$k$个不同的类别$y_1, y_2, ..., y_k$，分类任务等价于寻找下式：\n",
    "\n",
    "$$\n",
    "ans = \\textbf{argmax}_{i} P(y_i | X)\n",
    "$$\n",
    "\n",
    "朴素贝叶斯中假设$X=\\{a_1, a_2, ..., a_n\\}$为待分类项，$a_i$为$X$的一个属性，不同属性之间相互独立。\n",
    "由全概率公式可以得出：\n",
    "\n",
    "$$\n",
    "P(y_i | X) = \\frac{P(X, y_i)}{P(X)} = \\frac{P(X | y_i) * P(y_i)}{P(X)}\n",
    "$$\n",
    "\n",
    "由$X$不同属性相互独立可以得出：\n",
    "\n",
    "$$\n",
    "P(X|y_i) = \\prod_{j=1}^{n} P(a_j | y_i)\n",
    "$$\n",
    "\n",
    "而不同的$P(a_j | y_i)$我们可以通过训练数据统计得来。然后我们就可以计算出分子中的$P(X| y_i)$。\n",
    "进而，原始分类任务可以转化为：\n",
    "\n",
    "$$\n",
    "ans = \\textbf{argmax}_{i} \\frac{P(y_i) * \\prod_{j=1}^n P(a_j | y_i)}{P(X)}\n",
    "$$\n",
    "\n",
    "由于分母$P(X)$与不同$i$取值无关，可以忽略，故简化为：\n",
    "\n",
    "$$\n",
    "ans = \\textbf{argmax}_{i}( P(y_i) * \\prod_{j=1}^n P(a_j | y_i))\n",
    "$$\n",
    "\n",
    "### 核心代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBC:\n",
    "    \"\"\"\n",
    "    Naive Bayesian Classifier核心代码实现\n",
    "    注，这里做了部分偷懒。对于原始的a_j我们统一进行特征编号，这样对于某个y_i就可以采用一维数组存储P(a_j|y_i)\n",
    "    但这会导致P(a_j | y_i)的计算 self.P[y_i][a_j]/sum(self.P[y_i]) 与实际不符，应该采用 self.P[y_i][a_j]/self.classes_count[y_i]\n",
    "    \"\"\"\n",
    "    def __init__(self, max_values, classes):\n",
    "        \"\"\"\n",
    "        max_value_in_dims: 每个维度最大的可能取值范围，建立对应的数组。\n",
    "        classes：类别个数。\n",
    "        \"\"\"\n",
    "        self.max_values = max_values\n",
    "        self.classes = classes\n",
    "        self.P = [[0 for j in range(max_values)] for i in range(classes)]\n",
    "        self.classes_count = [0 for i in range(classes)]\n",
    "        self.total_samples = 0\n",
    "    \n",
    "    def update(self, train_data, train_label):\n",
    "        \"\"\"\n",
    "        train_data: list(list(数据中所含的特征))\n",
    "        train_label: list()\n",
    "        \"\"\"\n",
    "        N = len(train_label)\n",
    "        total_samples += N\n",
    "        for i in range(N):\n",
    "            for x in train_data[i]:\n",
    "                self.P[train_label[i]][x] += 1\n",
    "            self.classes_count[train_label[i]] += 1\n",
    "    \n",
    "    def test(self, test_data):\n",
    "        \"\"\"test_data: list()\"\"\"\n",
    "        max_label = -1\n",
    "        max_p = 0.0\n",
    "        for i in range(self.classes):\n",
    "            Pyi = self.classes_count[i]/total_samples\n",
    "            Ptot = Pyi\n",
    "            for x in test_data:\n",
    "                Pajyi = self.P[i][x]/self.classes_count[i]\n",
    "                Ptot *= Pajyi\n",
    "            \n",
    "            if max_label == -1 or Ptot > max_p:\n",
    "                max_p = Ptot\n",
    "                max_label = i\n",
    "        return i, max_p\n",
    "\"\"\"\n",
    "注，写的比较仓促\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展\n",
    "\n",
    "对于那些特征之间存在相关关系的属性集合，可以构建贝叶斯信念网络来计算。\n",
    "\n",
    "此外，由于数据中并不能包含所有情况，因此对于一个其他特征都符合属性$y_i$，但某个特征取值$a_k$恰好没有出现在训练集中的情况，采用上述计算方法会导致概率乘$P(a_k | y_i) = 0$。这显然不符合常理。可以引入**平滑方法**来解决上述问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后记\n",
    "\n",
    "谢谢大家！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
